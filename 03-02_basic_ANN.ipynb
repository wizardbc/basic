{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kHJSGlcJ-ubC"
      },
      "source": [
        "# Back to the Basic (Part 3-2. Artificial Neural Network)\n",
        "\n",
        "* 2023.05.22.(Mon)\n",
        "* Dept. of Math., Inha Univ.\n",
        "* Byung Chun Kim (wizardbc@gmail.com)\n",
        "\n",
        "# Artificial Neural Network (ANN)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BgEsABIe-yXb"
      },
      "source": [
        "![ANN](https://upload.wikimedia.org/wikipedia/commons/3/3d/Neural_network.svg)\n",
        "\n",
        "[Wikimedia](https://commons.wikimedia.org/wiki/File:Neural_network.svg)\n",
        "\n",
        "* Each circle represents a real number.\n",
        "* The input neurons (the green circles) represent a vector $(x_1, x_2)^T\\in\\mathbb R^2$.\n",
        "* The edges from the input neurons to the hidden neurons (the blue circles) represent weights $\\left(a_{ij}^{(1)}\\right)$ and biases $\\left(b_{ij}^{(1)}\\right)$ in $\\mathbb{R}^{5\\times 2}$.\n",
        "* The hidden neurons (the blue circles) represent a vector $\\left(\\sigma(y_1), \\ldots, \\sigma(y_5)\\right)^T\\in\\mathbb R^5$ where $y_i = \\sum_{j=1}^2\\left(a_{ij}^{(1)}x_j+b_{ij}^{(1)}\\right)$ and $\\sigma:\\mathbb R\\rightarrow\\mathbb R$ is an activation function.\n",
        "* The edges from the hidden neurons to output neurons (the yello circle) represent weights $\\left(a_{ij}^{(2)}\\right)$ and biases $\\left(b_{ij}^{(2)}\\right)$ in $\\mathbb{R}^{1\\times 5}$.\n",
        "* The output neurons represent a real number $\\sum_{j=1}^5 \\left(a_{ij}^{(2)}\\sigma(y_i)+b_{ij}^{(2)}\\right)$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pphj3why6TI"
      },
      "source": [
        "## Affine Maps\n",
        "\n",
        "If $x=(x_1,\\ldots, x_d)^T\\in\\mathbb R^d$, an affine map $W$ consists of linear map (matrix) $A=\\left(a_{ij}\\right)\\in\\mathbb R^{n\\times d}$ (weights) and vector $b=(b_1,\\ldots,b_n)^T\\in\\mathbb R^n$ (biases). Then $W(x)$ means $n$-dimensional vector $Ax+b=\\left(\\sum_{j=1}^d a_{ij}x_j+b_i\\right)^T\\in\\mathbb R^n$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5UpGSUT9y9Zi"
      },
      "source": [
        "## Component-wise Composition\n",
        "\n",
        "Given a function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$, and $y=\\left(y_1,\\ldots,y_n\\right)^T\\in\\mathbb R^n$, $\\sigma(y)$ means $\\left(\\sigma(y_1),\\ldots,\\sigma(y_n)\\right)^T\\in\\mathbb R^n$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A4cvy9VrnQCq"
      },
      "source": [
        "Now we can write the neural network in the picture by\n",
        "$$W_2\\cdot\\sigma\\circ W_1$$\n",
        "where\n",
        "*  $W_1 = \\left(A_1, b_1\\right)$ with $A_1=\\left(a_{ij}^{(1)}\\right)\\in\\mathbb R^{5\\times 2}$ and $b_1=\\left(\\sum_{j=1}^2 b_{ij}^{(1)}\\right)^T\\in\\mathbb R^5$,\n",
        "*  $W_2 = \\left(A_2, b_2\\right)$ with $A_2=\\left(a_{ij}^{(2)}\\right)\\in\\mathbb R^{1\\times 5}$ and $b_2=\\sum_{j=1}^5 b_{1j}^{(2)}\\in\\mathbb R^1$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9JRg6jUoBa"
      },
      "source": [
        "# Universal Approximation Theorem"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Co8g9fXVNn"
      },
      "source": [
        "## Arbitrary-width case\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-width_case)\n",
        "\n",
        "(1989, George Cybenko)\n",
        "\n",
        "Fix a continuous function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$ (activation function) and positive integers $d, D$. The function $\\sigma$ is not a polynomial if and only if, for every continuous function $f:\\mathbb R^d\\rightarrow\\mathbb R^D$ (target function), every compact subset $K$ of $\\mathbb R^d$, and every $\\epsilon > 0$ there exists a continuous function $f_\\epsilon:\\mathbb R^d\\rightarrow\\mathbb R^D$ with representation $$f_\\epsilon = W_2\\cdot\\sigma\\circ W_1,$$\n",
        "where $W_1, W_2$ are composable affine maps and $\\circ$ denotes component-wise composition, such that the approximation bound $$\\sup_{x\\in K}\\|f(x)-f_\\epsilon(x)\\|<\\epsilon.$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SVHN74K5YixU"
      },
      "source": [
        "### Sketch\n",
        "Given continuous $\\sigma:\\mathbb R\\rightarrow\\mathbb R$, positive integers $d, D$.\n",
        "\n",
        "$\\sigma$ is not a polynomial $\\iff$\n",
        "* $\\forall f:\\mathbb R^d\\rightarrow \\mathbb R^D$,\n",
        "* $\\forall$ compact $K\\subset \\mathbb R^d$,\n",
        "* $\\forall \\epsilon>0$,\n",
        "\n",
        "there exists a neural network $\\hat f$:\n",
        "* $d$ input neurons,\n",
        "* $D$ output neurons,\n",
        "* only one hidden layer with an arbitrary number of hidden neurons having activation function $\\sigma$,\n",
        "\n",
        "such that\n",
        "$$\\sup_{x\\in K}\\|f(x)-\\hat f(x)\\|<\\epsilon.$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ePLq5dzBfa"
      },
      "source": [
        "#### Unit Step Function (Heaviside Step Function)\n",
        "\n",
        "$$H(x) := \\begin{cases} 1, & x > 0 \\\\ 0, & x \\le 0 \\end{cases}$$\n",
        "Sometimes,\n",
        "$$H(x) := \\begin{cases} 1, & x > 0 \\\\ \\frac{1}{2}, & x = 0 \\\\ 0, & x < 0 \\end{cases}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubQUCin7P6Cr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXy4X9ziRmA4"
      },
      "outputs": [],
      "source": [
        "plt.style.use('dark_background')\n",
        "\n",
        "# H = np.vectorize(lambda x: 1 if x > 0.0 else 0.0)\n",
        "H = np.vectorize(lambda x: 1/2 if x==0.0 else 1 if x>0.0 else 0.0)\n",
        "\n",
        "domain = np.linspace(-10,10,1001)\n",
        "plt.scatter(domain, H(domain), s=.1)\n",
        "plt.title(\"Unit step function\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx43Q2HU70oR"
      },
      "source": [
        "#### Boxcar Function\n",
        "\n",
        "$$\\Pi_{a,b}(x):=H(x-a)-H(x-b)$$\n",
        "We can approximate any continuous function $f:\\mathbb R\\rightarrow\\mathbb R$ in some closed interval $K=[s,e]$ using boxcar functions:\n",
        "\n",
        "Given any $\\epsilon > 0$, we can find positive integer $n$ such that if $$x_i := s+\\frac{e-s}{n}i \\qquad \\left(0\\leq i \\leq n\\right)$$\n",
        "and\n",
        "$$f_n:=\\sum_{i=1}^{n}f(x_{i})\\Pi_{x_{i-1},x_{i}}$$\n",
        "then\n",
        "$$\\sup_{x\\in K}\\|f(x)-f_n(x)\\|<\\epsilon.$$\n",
        "\n",
        "Moreover, since\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "f_n &= \\sum_{i=1}^{n}f(x_i)\\left(H(x-x_{i-1}) - H(x-x_i)\\right)\\\\\n",
        "&=f(x_1)H(x-x_{0})-\\sum_{i=1}^{n-1}\\left(f(x_i)-f(x_{i+1})\\right)H(x-x_i)-f(x_n)H(x-x_n),\n",
        "\\end{align*}\n",
        "$$\n",
        "we have\n",
        "$$f_n = W_2\\cdot H\\circ W_1$$\n",
        "where\n",
        "$$\n",
        "\\begin{align*}\n",
        "W_1 &= \\left((1,1,\\ldots,1)^T, \\left(-x_0,-x_1,\\ldots,-x_n\\right)^T\\right)\\in\\mathbb R^{(n+1)\\times 1}\\times\\mathbb R^{n+1},\\\\\n",
        "W_2 &= \\left(\\left(f(x_1), f(x_2)-f(x_1), f(x_3)-f(x_2),\\ldots,f(x_n)-f(x_{n-1}), -f(x_n)\\right), 0\\right)\\in\\mathbb R^{1\\times (n+1)}\\times\\mathbb R.\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daV1Hv-CTawh"
      },
      "outputs": [],
      "source": [
        "boxcar = lambda x, a=-.5, b=.5: H(x-a) - H(x-b)\n",
        "\n",
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, boxcar(domain), s=.1)\n",
        "plt.title(\"Unit rectangle function\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHKGZIsk7XI4"
      },
      "outputs": [],
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.scatter(domain, f(domain), s=.1, label='$f$')\n",
        "  plt.scatter(domain, fn, s=.1, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.scatter(domain, err, s=.1)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sw5YNc9HNj7x"
      },
      "source": [
        "* We have approximated $f$ using a number of unit step functions.\n",
        "\n",
        "By replacing the closed interval $K$ to the product of closed intervals, we can show the general $f:\\mathbb R^d\\rightarrow\\mathbb R^D$ case.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sPBvUSXWr_49"
      },
      "source": [
        "#### Continuous Version of Unit Step Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkJVghGURPrr"
      },
      "outputs": [],
      "source": [
        "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "domain = np.linspace(-10,10,1001)\n",
        "\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "plt.plot(domain, sigmoid(domain), label='sigmoid')\n",
        "plt.title(\"Sigmoid\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ir7gw9DRkDO"
      },
      "outputs": [],
      "source": [
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "for a in [1, 2, 4, 8, 16, 32]:\n",
        "  plt.plot(domain, sigmoid(a*domain), label=f'a={a}')\n",
        "plt.title(\"Sigmoid(ax)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkU2MPG5Q3d1"
      },
      "outputs": [],
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "sig_boxcar = lambda x, a, b: sigmoid(512*(x-a)) - sigmoid(512*(x-b))\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*sig_boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(domain, f(domain), label='$f$')\n",
        "  plt.plot(domain, fn, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.plot(domain, err)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sB1UoEd2syqv"
      },
      "source": [
        "* We have approximated $f$ using a number of sigmoid functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T01fqX9xH3vg"
      },
      "outputs": [],
      "source": [
        "relu = lambda x: x*(x>0)\n",
        "\n",
        "domain = np.linspace(-10,10,1000)\n",
        "\n",
        "plt.plot(domain, relu(domain))\n",
        "plt.title(\"ReLU\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM6zuCCkyECH"
      },
      "outputs": [],
      "source": [
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "for a in [1, 2, 4, 8, 16, 32]:\n",
        "  plt.plot(domain, relu(a*domain+0.5) - relu(a*domain-0.5), label=f'a={a}')\n",
        "plt.title(\"ReLU(ax+0.5)-ReLU(ax-0.5)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_EA6UTyoaD"
      },
      "outputs": [],
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "relu_boxcar = lambda x, a, b: (relu(256*(x-a)+0.5) - relu(256*(x-a)-0.5)) - (relu(256*(x-b)+0.5) - relu(256*(x-b)-0.5))\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*relu_boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(domain, f(domain), label='$f$')\n",
        "  plt.plot(domain, fn, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.plot(domain, err)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JvYLLg7es17Y"
      },
      "source": [
        "* We have approximated $f$ using a number of ReLU(Rectified Linear Unit) functions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VatA_HB2bWG3"
      },
      "source": [
        "#### Summary\n",
        "* continuous function $f:\\mathbb R^d\\rightarrow\\mathbb R^D$\n",
        "* continuous non-polynomial function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$.\n",
        "\n",
        "Then $f$ can be approximated by a neural network represented by $W_2\\circ\\sigma\\circ W_1$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DIt1JWAGjXCE"
      },
      "source": [
        "## Arbitrary-depth case\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-depth_case)\n",
        "\n",
        "(2017, Zhou Lu et al.)\n",
        "\n",
        "Let $\\mathcal{X}$ be a compact subset of $\\mathbb{R}^d$. Let $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ be any non-affine continuous function which is continuously differentiable at at-least one point, with non-zero derivative at that point. Let $\\mathcal{N}_{d,D:d+D+2}^{\\sigma}$ denote the space of feed-forward neural networks with $d$ input neurons, $D$ output neurons, and an arbitrary number of hidden layers each with $d + D + 2$ neurons, such that every hidden neuron has activation function $\\sigma$ and every output neuron has the identity as its activation function, with input layer $\\phi$, and output layer $\\rho$. Then given any $\\varepsilon>0$ and any $f\\in C(\\mathcal{X},\\mathbb{R}^D)$, there exists $\\hat{f}\\in \\mathcal{N}_{d,D:d+D+2}^{\\sigma}$ such that\n",
        "$$\n",
        "\\sup_{x \\in \\mathcal{X}}\\,\\left\\|\\hat{f}(x)-f(x)\\right\\| < \\varepsilon.\n",
        "$$\n",
        "\n",
        "In other words, $\\mathcal{N}$ is dense in $C(\\mathcal{X}; \\mathbb{R}^D)$ with respect to the uniform topology.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfdZadkkzDo"
      },
      "source": [
        "### Sketch\n",
        "Given a continuous function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$\n",
        "\n",
        "$\\sigma$ is continuously differentiable at at-least one point, with non-zero derivative at that point\n",
        "\n",
        "$\\implies$\n",
        "* $\\forall f:\\mathbb R^d\\rightarrow \\mathbb R^D$,\n",
        "* $\\forall$ compact $\\mathcal X\\subset \\mathbb R^d$,\n",
        "* $\\forall \\epsilon>0$,\n",
        "\n",
        "there exists a neural network $\\hat f$:\n",
        "* $d$ input neurons,\n",
        "* $D$ output neurons,\n",
        "* an arbitrary number of hidden layers each with $d+D+2$ neurons having activation function $\\sigma$,\n",
        "\n",
        "satisfying\n",
        "$$\\sup_{x\\in\\mathcal X}\\|f(x)-\\hat f(x)\\|<\\epsilon.$$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NehxEhp4YfIP"
      },
      "source": [
        "# Some Lessons from Universal Approximation Theorem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugzWAQ3BXzTj"
      },
      "outputs": [],
      "source": [
        "# from functools import partial\n",
        "\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "# from jax import random\n",
        "# from jax import jax\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# print(\"Devices:\", jax.devices())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4i1JPosiaFe7"
      },
      "source": [
        "## Real Valued Function\n",
        "\n",
        "* $[0,1]$ part of cubic polynomial $$f(x)=-x(x-1)(x+1)$$\n",
        "\n",
        "* Our data is $$\\left\\{\\left(x_1, f(x_1)+\\epsilon_1\\right),\\ldots, \\left(x_{100}, f(x_{100})+\\epsilon_{100}\\right)\\right\\}$$\n",
        "where $\\epsilon_i$ are noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = lambda x: -x*(x-1)*(x+1)\n",
        "x = np.linspace(0,1,1000).reshape(1000,1)\n",
        "\n",
        "xs = np.random.uniform(size=(100,1))\n",
        "ys = f(xs) #+ np.random.normal(size=(100,1))*0.01\n",
        "\n",
        "plt.title('Train Dataset')\n",
        "plt.plot(x, f(x), label='$y=f(x)$', c='red')\n",
        "plt.scatter(xs, ys, s=10, label='data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "* Model\n",
        "  * Input dim. = 1\n",
        "  * Hidden dim. = 50\n",
        "  * Output dim. = 1\n",
        "* Linear layer\n",
        "  * input, $x = (x_{j})$,\n",
        "  * weight, $W = (w_{ij})$,\n",
        "  * bias, $b = (b_i)$,\n",
        "  * result, $a = (a_i) = W @ x + b$,\n",
        "  $$a_i = \\sum_{j} w_{ij} x_j + b_i$$\n",
        "  * gradient,\n",
        "  $$\\frac{\\partial}{\\partial w_{ij}}a_i = x_j$$\n",
        "  $$\\frac{\\partial}{\\partial b_{i}}a_i = 1$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Module:\n",
        "  def forward(self, x:np.array) -> np.array:\n",
        "    raise NotImplementedError\n",
        "  def backward(self, grad:np.array) -> np.array:\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sigmoid(Module):\n",
        "  def forward(self, x):\n",
        "    self._x = x\n",
        "    return 1 / (2 + np.exp(-x))\n",
        "  \n",
        "  def backward(self, grad: np.array) -> np.array:\n",
        "    s = self.forward(self._x)\n",
        "    return grad*s*(1-s)\n",
        "\n",
        "class ReLU(Module):\n",
        "  def forward(self, x:np.array) -> np.array:\n",
        "    self._x = x\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "  def backward(self, grad: np.array) -> np.array:\n",
        "    return (self._x > 0) * grad\n",
        "  \n",
        "class MSE(Module):\n",
        "  def forward(self, y_pred:np.array, y_true:np.array) -> np.array:\n",
        "    self._pred = y_pred\n",
        "    self._true = y_true\n",
        "    return ((y_pred - y_true)**2).mean()\n",
        "  \n",
        "  def backward(self) -> np.array:\n",
        "    return 2*(self._pred - self._true)/self._true.size\n",
        "  \n",
        "class Linear(Module):\n",
        "  def __init__(self, input_dim:int, output_dim:int) -> None:\n",
        "    self.weight = np.random.randn(input_dim, output_dim)\n",
        "    self.bias = np.zeros(output_dim)\n",
        "\n",
        "  def forward(self, x:np.array) -> np.array:\n",
        "    self._x = x\n",
        "    return x @ self.weight + self.bias\n",
        "  \n",
        "  def backward(self, grad:np.array) -> np.array:\n",
        "    self._d_weight = self._x.T @ grad\n",
        "    self._d_bias = np.sum(grad, axis=0)\n",
        "    return grad @ self.weight.T\n",
        "  \n",
        "  def update(self, lr:float) -> None:\n",
        "    self.weight -= lr * self._d_weight\n",
        "    self.bias -= lr * self._d_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(Module):\n",
        "  def __init__(self, input_dim:int, hidden_dim:int, output_dim:int) -> None:\n",
        "    self.fc1 = Linear(input_dim, hidden_dim)\n",
        "    self.activation = ReLU()\n",
        "    self.fc2 = Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x:np.array) -> np.array:\n",
        "    x = self.fc1.forward(x)\n",
        "    x = self.activation.forward(x)\n",
        "    x = self.fc2.forward(x)\n",
        "    return x\n",
        "  \n",
        "  def backward(self, grad:np.array) -> None:\n",
        "    grad = self.fc2.backward(grad)\n",
        "    grad = self.activation.backward(grad)\n",
        "    self.fc1.backward(grad)\n",
        "\n",
        "  def update(self, lr:float) -> None:\n",
        "    self.fc1.update(lr)\n",
        "    self.fc2.update(lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NeuralNetwork(1,20,1)\n",
        "loss_fn = MSE()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(200000):\n",
        "  y_pred = model.forward(xs)\n",
        "  loss = loss_fn.forward(y_pred, ys)\n",
        "\n",
        "  if i%1000==0:\n",
        "    print(f'Epoch : {i}\\tloss:{loss:.8f}')\n",
        "\n",
        "  model.backward(loss_fn.backward())\n",
        "  model.update(0.01)\n",
        "\n",
        "print(f'Epoch : {i}\\tloss:{loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.title('After Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs, ys, s=10)\n",
        "\n",
        "plt.plot(x, model.forward(x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs, model.forward(xs), s=10)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ03EA0nhfDS"
      },
      "source": [
        "* Good Job! (Really?)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dM68JLRfiKRr"
      },
      "source": [
        "## Lesson 1\n",
        "\n",
        "ANN model CANNOT tell about outside of the comapct set - in this case, outside of $[0,1]$.\n",
        "\n",
        "Note that the universal approximation theorem says:<br>\n",
        "the approximation bound\n",
        "$$\\sup_{x\\in K}\\|f(x)-f_\\epsilon(x)\\|<\\epsilon$$\n",
        "where $K\\subset\\mathbb{R}^D$ is a compact subset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNs6IpIwaHdu"
      },
      "outputs": [],
      "source": [
        "plt.title('Outside of the campact set')\n",
        "xx = np.linspace(-1,1.5,2500)[:, np.newaxis]\n",
        "plt.plot(xx, f(xx),label='$y=f(x)$')\n",
        "plt.scatter(xs, ys, s=1)\n",
        "\n",
        "plt.plot(xx, model.forward(xx), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs, model.forward(xs), s=1)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_D6sXBiSmJk4"
      },
      "source": [
        "## Lesson 2\n",
        "\n",
        "You should have enough data.\n",
        "\n",
        "If we have only 4 points of the cubic polynomial, then we get a bad approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SNmz6TimeGB"
      },
      "outputs": [],
      "source": [
        "f = lambda x: -x*(x-1)*(x+1)\n",
        "x = np.linspace(0,1,1000).reshape(1000,1)\n",
        "\n",
        "xs2 = np.random.uniform(size=(4,1))\n",
        "ys2 = f(xs2)# + np.random.normal(size=(4,1))*0.01\n",
        "\n",
        "plt.title('Train Dataset')\n",
        "plt.scatter(xs2, ys2, s=10, label='data')\n",
        "plt.plot(x, f(x), label='$y=f(x)$', c='red')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = NeuralNetwork(1,20,1)\n",
        "loss_fn = MSE()\n",
        "\n",
        "for i in range(200000):\n",
        "  y_pred = model2.forward(xs2)\n",
        "  loss = loss_fn.forward(y_pred, ys2)\n",
        "\n",
        "  if i%1000==0:\n",
        "    print(f'Epoch : {i}\\tloss:{loss:.8f}')\n",
        "\n",
        "  model2.backward(loss_fn.backward())\n",
        "  model2.update(0.01)\n",
        "\n",
        "print(f'Epoch : {i}\\tloss:{loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjB9e5M-t1Z"
      },
      "outputs": [],
      "source": [
        "plt.title('After Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2, s=10)\n",
        "\n",
        "plt.plot(x, model2.forward(x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs2, model2.forward(xs2), s=10)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1YJuC5nRcY"
      },
      "source": [
        "## Lesson 3\n",
        "\n",
        "***First of all***, you have to check if there exists a exact solution for the problem.\n",
        "\n",
        "Because we are working on cubic polynomial, 4 points are enough.\n",
        "\n",
        "Just solve the equation:\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "x_1^3 & x_1^2 & x_1 & 1\\\\\n",
        "x_2^3 & x_2^2 & x_2 & 1\\\\\n",
        "x_3^3 & x_3^2 & x_3 & 1\\\\\n",
        "x_4^3 & x_4^2 & x_4 & 1\\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "a\\\\ b\\\\ c\\\\ d\\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\y_2 \\\\y_3 \\\\y_4 \\\\\n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKRQ3_Tn6bya"
      },
      "outputs": [],
      "source": [
        "A = np.concatenate([xs2**i for i in [3,2,1,0]], axis=1)\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVY00xg7VQI"
      },
      "outputs": [],
      "source": [
        "a,b,c,d = np.linalg.inv(A) @ ys2\n",
        "\n",
        "g = lambda x: a*x**3 + b*x**2 + c*x + d\n",
        "\n",
        "print(f'{a.item():.4f}x^3 + {b.item():.4f}x^2 + {c.item():.4f}x + {d.item():.4f}')\n",
        "\n",
        "plt.title('Exact solution')\n",
        "plt.plot(xx, f(xx),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2, s=10)\n",
        "\n",
        "plt.plot(xx, g(xx), label='$y=g(x)$')\n",
        "plt.scatter(xs2, g(xs2), s=10)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta9ptQOzpHFI"
      },
      "source": [
        "These lessons are just the first stpe of your journey to data science.\n",
        "\n",
        "By just reading the statement of the universal approximation theorem,\n",
        "you can figure out many strenghs and weaknesses of ANN.\n",
        "\n",
        "Good luck.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "JAX_ANN.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
